{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download data\n",
    "Download data here https://www.kaggle.com/datasets/mikhailgaerlan/spongebob-squarepants-completed-transcripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "folder_path = \"SpongeBob\"\n",
    "\n",
    "sponge_data = []\n",
    "# Loop over all the files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "    with open(file_path, \"r\") as file:\n",
    "        document = []\n",
    "        for line in file:\n",
    "            clean_string = re.sub(r'\\[.*?\\]', '', line).strip()\n",
    "            if clean_string:\n",
    "                character, *utterance = clean_string.split(\":\")\n",
    "                utterance = ':'.join(utterance).strip()\n",
    "                character = character.strip()\n",
    "                if utterance:\n",
    "                    # Truncate up to 20 words\n",
    "                    words = utterance.split()\n",
    "                    trunc_utter = ' '.join(words[:20]) \n",
    "                    document.append({\"utterance\": trunc_utter, \"character\": character})\n",
    "    sponge_data.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_consecutive_utters_with_same_character(df):\n",
    "    merged_data = []\n",
    "    last_utter = None\n",
    "    for doc in sponge_data:\n",
    "        document = []\n",
    "        for utterance in doc:\n",
    "            if last_utter is not None and utterance['character'] == last_utter['character']:\n",
    "                last_utter['utterance'] += ' ' + utterance['utterance']\n",
    "            else:\n",
    "                if last_utter is not None:\n",
    "                    document.append(last_utter)\n",
    "                last_utter = utterance.copy()\n",
    "        if last_utter is not None:\n",
    "            document.append(last_utter)\n",
    "        merged_data.append(document)\n",
    "    return merged_data\n",
    "\n",
    "sponge_data = merge_consecutive_utters_with_same_character(sponge_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download personachat dataset\n",
    "# !wget https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "with open(\"personachat_self_original.json\", 'r') as f:\n",
    "    persona_data = json.load(f)\n",
    "\n",
    "\n",
    "def get_random_sentences(n=5):\n",
    "    sentences = []\n",
    "    for i in range(n):\n",
    "        random_persona = random.choice(persona_data['train'])\n",
    "        random_utterance = random.choice(random_persona['utterances'])\n",
    "        random_sentence = random.choice(random_utterance['candidates'])\n",
    "        sentences.append(random_sentence)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = {}\n",
    "\n",
    "from personality import sponge_bob_personality\n",
    "\n",
    "dataset['personality'] = sponge_bob_personality\n",
    "dataset[\"utterances\"] = []\n",
    "\n",
    "SPONGE_BOB = 'SpongeBob'\n",
    "last_utter = None\n",
    "history = []\n",
    "for doc in sponge_data:\n",
    "    for utterance in doc:\n",
    "        if last_utter is not None and last_utter['character'] != SPONGE_BOB and utterance['character'] != SPONGE_BOB:\n",
    "            history = []\n",
    "\n",
    "        if history and utterance['character'] == SPONGE_BOB:\n",
    "            example = {}\n",
    "            example['candidates'] = get_random_sentences()\n",
    "            example['candidates'].append(utterance['utterance'])\n",
    "            # Take no more than 5 utterances\n",
    "            example['history'] = history.copy()[-5:]\n",
    "            dataset['utterances'].append(example)\n",
    "\n",
    "        history.append(utterance['utterance'])\n",
    "        last_utter = utterance.copy()\n",
    "    history = []\n",
    "    last_utter = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12023"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset[\"utterances\"])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split to train and test dataset for GPT chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(dataset['utterances'], test_size=0.1)\n",
    "\n",
    "train_dataset = {}\n",
    "train_dataset['personality'] = sponge_bob_personality\n",
    "train_dataset[\"utterances\"] = train_data\n",
    "\n",
    "with open('train.json', 'w') as f:\n",
    "    json.dump([train_dataset], f)\n",
    "\n",
    "\n",
    "test_dataset = {}\n",
    "test_dataset['personality'] = sponge_bob_personality\n",
    "test_dataset[\"utterances\"] = test_data\n",
    "\n",
    "with open('test.json', 'w') as f:\n",
    "    json.dump([test_dataset], f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data file for simple chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sponge_bob_statements.txt\", 'w') as f:\n",
    "    for utterance in dataset[\"utterances\"]:\n",
    "        f.write(f\"{utterance['candidates'][-1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open('sponge_bob_statements.txt', 'r') as f1:\n",
    "    content1 = f1.read()\n",
    "\n",
    "with open('../personachat_train_statements.txt', 'r') as f2:\n",
    "    content2 = f2.read()\n",
    "\n",
    "# Concatenate the contents of the two files\n",
    "content = content1 + \"\\n\" + content2\n",
    "\n",
    "# Open the third file in write mode and write the concatenated contents to it\n",
    "with open('personachat_sponge_bob_statements.txt', 'w') as f3:\n",
    "    f3.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
